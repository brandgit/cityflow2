"""
Point d'entr√©e principal - Orchestration compl√®te du traitement des donn√©es CityFlow Analytics
Ex√©cut√© dans AWS (Lambda/EC2) pour le preprocessing
"""

import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Ajouter le r√©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

# Imports configuration
from config import settings

# Imports processeurs
from processors import (
    BikesProcessor, TrafficProcessor, WeatherProcessor,
    ComptagesProcessor, ChantiersProcessor, ReferentielProcessor
)

# Imports utilitaires (depuis processors/utils/)
from processors.utils.file_utils import (
    load_json, find_json_files, load_and_combine_json_files, find_csv_files
)

# Import services base de donn√©es (MongoDB ou DynamoDB)
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.database_factory import get_database_service, get_database_type
from utils.metrics_optimizer import should_optimize_for_mongodb, optimize_metrics_for_storage

# Import services AWS S3
from utils.aws_services import (
    download_s3_directory,
    download_s3_file_to_temp,
    list_s3_files,
    load_json_from_s3
)


def load_raw_data_from_s3(config) -> Dict[str, Any]:
    """
    Charge les donn√©es brutes depuis S3 (mode AWS)
    
    Args:
        config: Configuration
    
    Returns:
        Dict avec toutes les donn√©es brutes par type
    """
    print("\n‚òÅÔ∏è  Mode AWS d√©tect√© - T√©l√©chargement depuis S3...")
    
    raw_data = {
        "bikes": None,
        "traffic": None,
        "weather": None,
        "comptages": None,
        "chantiers": None,
        "referentiel": None
    }
    
    # Cr√©er r√©pertoire de cache local pour S3
    cache_dir = Path(config.S3_CACHE_DIR)
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    bucket_name = config.S3_RAW_BUCKET
    s3_prefix = config.S3_RAW_PREFIX
    
    try:
        # === Charger donn√©es API (JSON) depuis S3 ===
        
        # Bikes (structure: api/bikes/dt=YYYY-MM-DD/)
        bikes_s3_prefix = f"{s3_prefix}/api/bikes/dt={config.API_DATE}/"
        print(f"üì• T√©l√©chargement bikes depuis S3://{bucket_name}/{bikes_s3_prefix}")
        bikes_files = download_s3_directory(
            bucket_name, 
            bikes_s3_prefix, 
            str(cache_dir / "bikes"),
            extensions=[".json", ".jsonl"]
        )
        if bikes_files:
            if len(bikes_files) > 1:
                raw_data["bikes"] = load_and_combine_json_files(bikes_files)
            else:
                raw_data["bikes"] = load_json(bikes_files[0])
        
        # Traffic (structure: api/traffic/dt=YYYY-MM-DD/)
        traffic_s3_prefix = f"{s3_prefix}/api/traffic/dt={config.API_DATE}/"
        print(f"üì• T√©l√©chargement traffic depuis S3://{bucket_name}/{traffic_s3_prefix}")
        traffic_files = download_s3_directory(
            bucket_name,
            traffic_s3_prefix,
            str(cache_dir / "traffic"),
            extensions=[".json", ".jsonl"]
        )
        if traffic_files:
            if len(traffic_files) > 1:
                raw_data["traffic"] = load_and_combine_json_files(traffic_files)
            else:
                raw_data["traffic"] = load_json(traffic_files[0])
        
        # Weather (structure: api/weather/dt=YYYY-MM-DD/)
        weather_s3_prefix = f"{s3_prefix}/api/weather/dt={config.API_DATE}/"
        print(f"üì• T√©l√©chargement weather depuis S3://{bucket_name}/{weather_s3_prefix}")
        weather_files = download_s3_directory(
            bucket_name,
            weather_s3_prefix,
            str(cache_dir / "weather"),
            extensions=[".json", ".jsonl"]
        )
        if weather_files:
            if len(weather_files) > 1:
                raw_data["weather"] = load_and_combine_json_files(weather_files)
            else:
                raw_data["weather"] = load_json(weather_files[0])
        
        # === Charger donn√©es Batch (CSV) depuis S3 ===
        
        # Comptages
        comptages_s3_prefix = f"{s3_prefix}/batch/"
        print(f"üì• Recherche comptages dans S3://{bucket_name}/{comptages_s3_prefix}")
        comptages_files = list_s3_files(bucket_name, comptages_s3_prefix, extension=".csv")
        comptages_files = [f for f in comptages_files if "comptages" in f.lower()]
        if comptages_files:
            print(f"   Trouv√© {len(comptages_files)} fichier(s) comptages")
            # T√©l√©charger le premier fichier
            local_path = download_s3_file_to_temp(
                bucket_name,
                comptages_files[0],
                str(cache_dir / "batch")
            )
            if local_path:
                raw_data["comptages"] = local_path
        
        # Chantiers
        print(f"üì• Recherche chantiers dans S3://{bucket_name}/{comptages_s3_prefix}")
        chantiers_files = list_s3_files(bucket_name, comptages_s3_prefix, extension=".csv")
        chantiers_files = [f for f in chantiers_files if "chantiers" in f.lower()]
        if chantiers_files:
            print(f"   Trouv√© {len(chantiers_files)} fichier(s) chantiers")
            local_path = download_s3_file_to_temp(
                bucket_name,
                chantiers_files[0],
                str(cache_dir / "batch")
            )
            if local_path:
                raw_data["chantiers"] = local_path
        
        # R√©f√©rentiel
        print(f"üì• Recherche r√©f√©rentiel dans S3://{bucket_name}/{comptages_s3_prefix}")
        referentiel_files = list_s3_files(bucket_name, comptages_s3_prefix, extension=".csv")
        referentiel_files = [f for f in referentiel_files if "referentiel" in f.lower()]
        if referentiel_files:
            print(f"   Trouv√© {len(referentiel_files)} fichier(s) r√©f√©rentiel")
            local_path = download_s3_file_to_temp(
                bucket_name,
                referentiel_files[0],
                str(cache_dir / "batch")
            )
            if local_path:
                raw_data["referentiel"] = local_path
        
        print("\n‚úì T√©l√©chargement depuis S3 termin√©")
        
    except Exception as e:
        print(f"\n‚ö† Erreur lors du t√©l√©chargement depuis S3: {e}")
        print("   Tentative de chargement depuis fichiers locaux en fallback...")
        return load_raw_data_from_local(config)
    
    return raw_data


def load_raw_data_from_local(config) -> Dict[str, Any]:
    """
    Charge toutes les donn√©es brutes depuis les fichiers locaux (mode d√©veloppement)
    
    Args:
        config: Configuration
    
    Returns:
        Dict avec toutes les donn√©es brutes par type
    """
    print("\nüè† Mode Local d√©tect√© - Lecture depuis fichiers locaux...")
    
    raw_data = {
        "bikes": None,
        "traffic": None,
        "weather": None,
        "comptages": None,
        "chantiers": None,
        "referentiel": None
    }
    
    # Charger donn√©es API (JSON)
    try:
        # Bikes - Charger TOUS les fichiers et les combiner
        bikes_files = find_json_files(str(config.BIKES_JSON_PATH))
        if bikes_files:
            print(f"üìÅ Trouv√© {len(bikes_files)} fichier(s) bikes")
            if len(bikes_files) > 1:
                print(f"  ‚Üí Combinaison de {len(bikes_files)} fichiers...")
                raw_data["bikes"] = load_and_combine_json_files(bikes_files)
            else:
                raw_data["bikes"] = load_json(bikes_files[0])
        
        # Traffic - Charger TOUS les fichiers et les combiner
        traffic_files = find_json_files(str(config.TRAFFIC_JSON_PATH))
        if traffic_files:
            print(f"üìÅ Trouv√© {len(traffic_files)} fichier(s) traffic")
            if len(traffic_files) > 1:
                print(f"  ‚Üí Combinaison de {len(traffic_files)} fichiers...")
                raw_data["traffic"] = load_and_combine_json_files(traffic_files)
            else:
                raw_data["traffic"] = load_json(traffic_files[0])
        
        # Weather - Charger TOUS les fichiers et les combiner
        weather_files = find_json_files(str(config.WEATHER_JSON_PATH))
        if weather_files:
            print(f"üìÅ Trouv√© {len(weather_files)} fichier(s) weather")
            if len(weather_files) > 1:
                print(f"  ‚Üí Combinaison de {len(weather_files)} fichiers...")
                raw_data["weather"] = load_and_combine_json_files(weather_files)
            else:
                raw_data["weather"] = load_json(weather_files[0])
    except Exception as e:
        print(f"Erreur chargement donn√©es API: {e}")
    
    # Charger donn√©es Batch (CSV)
    try:
        # Comptages - Chercher tous les fichiers CSV dans le r√©pertoire
        comptages_dir = config.COMPTAGES_CSV.parent
        comptages_files = find_csv_files(str(comptages_dir), "comptages*.csv")
        if comptages_files:
            print(f"üìÅ Trouv√© {len(comptages_files)} fichier(s) comptages")
            if len(comptages_files) > 1:
                print(f"  ‚ö† Plusieurs fichiers trouv√©s, utilisation du premier: {comptages_files[0]}")
                print(f"  üí° Pour traiter plusieurs fichiers, utilisez le traitement par chunk")
            raw_data["comptages"] = comptages_files[0]  # Utiliser le premier pour compatibilit√©
        elif config.COMPTAGES_CSV.exists():
            raw_data["comptages"] = str(config.COMPTAGES_CSV)
        
        # Chantiers - Chercher tous les fichiers CSV
        chantiers_dir = config.CHANTIERS_CSV.parent
        chantiers_files = find_csv_files(str(chantiers_dir), "chantiers*.csv")
        if chantiers_files:
            print(f"üìÅ Trouv√© {len(chantiers_files)} fichier(s) chantiers")
            if len(chantiers_files) > 1:
                print(f"  ‚ö† Plusieurs fichiers trouv√©s, utilisation du premier: {chantiers_files[0]}")
            raw_data["chantiers"] = chantiers_files[0]
        elif config.CHANTIERS_CSV.exists():
            raw_data["chantiers"] = str(config.CHANTIERS_CSV)
        
        # R√©f√©rentiel - Chercher tous les fichiers CSV
        referentiel_dir = config.REFERENTIEL_CSV.parent
        referentiel_files = find_csv_files(str(referentiel_dir), "referentiel*.csv")
        if referentiel_files:
            print(f"üìÅ Trouv√© {len(referentiel_files)} fichier(s) r√©f√©rentiel")
            if len(referentiel_files) > 1:
                print(f"  ‚ö† Plusieurs fichiers trouv√©s, utilisation du premier: {referentiel_files[0]}")
            raw_data["referentiel"] = referentiel_files[0]
        elif config.REFERENTIEL_CSV.exists():
            raw_data["referentiel"] = str(config.REFERENTIEL_CSV)
    except Exception as e:
        print(f"Erreur chargement donn√©es batch: {e}")
    
    return raw_data


def load_raw_data(config) -> Dict[str, Any]:
    """
    Charge toutes les donn√©es brutes depuis S3 (AWS) ou local (d√©veloppement)
    D√©tection automatique selon l'environnement
    
    Args:
        config: Configuration
    
    Returns:
        Dict avec toutes les donn√©es brutes par type
    """
    # D√©tecter l'environnement
    is_aws = os.getenv("AWS_EXECUTION_ENV") is not None
    use_s3 = config.USE_S3 if hasattr(config, 'USE_S3') else False
    
    if is_aws or use_s3:
        # Mode AWS : T√©l√©charger depuis S3
        return load_raw_data_from_s3(config)
    else:
        # Mode Local : Lire depuis fichiers locaux
        return load_raw_data_from_local(config)


def initialize_processors(config) -> Dict[str, Any]:
    """
    Initialise tous les processeurs
    
    Args:
        config: Configuration
    
    Returns:
        Dict des processeurs par type
    """
    return {
        "bikes": BikesProcessor(config),
        "traffic": TrafficProcessor(config),
        "weather": WeatherProcessor(config),
        "comptages": ComptagesProcessor(config),
        "chantiers": ChantiersProcessor(config),
        "referentiel": ReferentielProcessor(config)
    }


def enrich_multi_source(results: Dict, referentiel_data: Optional[Dict] = None) -> Dict:
    """
    Enrichit les r√©sultats avec jointures multi-sources
    
    Args:
        results: R√©sultats de traitement
        referentiel_data: Donn√©es r√©f√©rentiel pour enrichissement
    
    Returns:
        R√©sultats enrichis
    """
    # Enrichir comptages avec r√©f√©rentiel
    # (Simplifi√© - dans un vrai projet, utiliser referentiel_data pour enrichir)
    if referentiel_data and "comptages" in results:
        # TODO: Utiliser referentiel_data pour enrichir les m√©triques comptages
        # avec libelles et m√©tadonn√©es g√©ographiques
        pass
    
    # Enrichir avec chantiers (intersection g√©ographique)
    # (Simplifi√© - n√©cessiterait calcul intersections g√©ographiques)
    
    return results


def cleanup_processed_chunks(config, keep_chunks=False):
    """
    Nettoie les fichiers chunks temporaires apr√®s traitement
    
    Args:
        config: Configuration
        keep_chunks: Si True, garde les chunks (pour debug)
    """
    if keep_chunks:
        return
    
    import glob
    import os
    
    chunk_files = glob.glob(str(config.PROCESSED_DIR / "*_chunk_*.csv"))
    
    if chunk_files:
        deleted_count = 0
        for chunk_file in chunk_files:
            try:
                os.remove(chunk_file)
                deleted_count += 1
            except Exception as e:
                print(f"  ‚ö† Erreur suppression {chunk_file}: {e}")
        
        if deleted_count > 0:
            print(f"  ‚úì {deleted_count} fichiers chunks nettoy√©s")


def export_results(results: Dict, config, date: Optional[str] = None) -> None:
    """
    Exporte les m√©triques calcul√©es vers la base de donn√©es (MongoDB ou DynamoDB)
    
    Args:
        results: R√©sultats de traitement
        config: Configuration
        date: Date au format YYYY-MM-DD (d√©faut: aujourd'hui)
    """
    # D√©terminer la date
    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")
    
    # Remplir la date dans toutes les m√©triques
    for data_type, result in results.items():
        if result and result.get("indicators"):
            indicators = result.get("indicators", {})
            # Remplir date dans les m√©triques individuelles
            if "metrics" in indicators and isinstance(indicators["metrics"], list):
                for metric in indicators["metrics"]:
                    # V√©rifier que metric est bien un dict
                    if isinstance(metric, dict) and "date" in metric and metric["date"] == "":
                        metric["date"] = date
            
            # Remplir date dans les top 10 tron√ßons
            if "top_10_troncons" in indicators:
                for troncon in indicators["top_10_troncons"]:
                    if "date" in troncon and troncon["date"] == "":
                        troncon["date"] = date
            
            # Remplir date dans les top 10 zones congestionn√©es
            if "top_10_zones_congestionnees" in indicators:
                for zone in indicators["top_10_zones_congestionnees"]:
                    if "date" in zone and zone["date"] == "":
                        zone["date"] = date
                    # S'assurer que zone_fallback est pr√©sent
                    if "zone_fallback" not in zone or not zone.get("zone_fallback"):
                        arr = zone.get("arrondissement", "Unknown")
                        if arr != "Unknown":
                            zone["zone_fallback"] = f"Arrondissement {arr}"
                        else:
                            zone["zone_fallback"] = "Unknown"
            
            # Remplir date dans les alertes de congestion
            if "alertes_congestion" in indicators:
                for alerte in indicators["alertes_congestion"]:
                    if "date" in alerte and alerte["date"] == "":
                        alerte["date"] = date
    
    # Obtenir le service de base de donn√©es (MongoDB ou DynamoDB selon config)
    try:
        db_service = get_database_service()
        db_type = get_database_type()
    except Exception as e:
        print(f"\n‚úó Erreur initialisation base de donn√©es: {e}")
        print("üí° Les m√©triques seront sauvegard√©es en local uniquement")
        db_service = None
        db_type = "local"
    
    # Exporter m√©triques par type
    exported_count = 0
    for data_type, result in results.items():
        if result and result.get("success"):
            indicators = result.get("indicators", {})
            if indicators:
                # V√©rifier si optimisation n√©cessaire pour MongoDB
                if db_service and should_optimize_for_mongodb(data_type, indicators):
                    # Cr√©er version optimis√©e pour MongoDB (sans liste compl√®te des tron√ßons)
                    optimized_indicators = optimize_metrics_for_storage(data_type, indicators)
                    print(f"  ‚ö† M√©triques {data_type} optimis√©es pour stockage (taille r√©duite)")
                    print(f"     ‚Üí Version compl√®te disponible en fichier local uniquement")
                    
                    # Sauvegarder version optimis√©e dans la base de donn√©es
                    success = db_service.save_metrics(
                        metrics=optimized_indicators,
                        data_type=data_type,
                        date=date
                    )
                    if success:
                        exported_count += 1
                        print(f"‚úì M√©triques {data_type} (summary) export√©es vers {db_type.upper()}")
                    else:
                        print(f"‚úó Erreur export m√©triques {data_type} vers {db_type.upper()}")
                else:
                    # Sauvegarder version compl√®te dans la base de donn√©es
                    if db_service:
                        success = db_service.save_metrics(
                            metrics=indicators,
                            data_type=data_type,
                            date=date
                        )
                        if success:
                            exported_count += 1
                            print(f"‚úì M√©triques {data_type} export√©es vers {db_type.upper()}")
                        else:
                            print(f"‚úó Erreur export m√©triques {data_type} vers {db_type.upper()}")
                
                # Toujours sauvegarder version compl√®te en local (backup + r√©f√©rence)
                if not os.getenv("AWS_EXECUTION_ENV"):
                    from processors.utils.file_utils import save_json
                    output_path = config.METRICS_DIR / f"{data_type}_metrics_{date}.json"
                    save_json(indicators, str(output_path))
                    print(f"  ‚Üí Sauvegarde locale (backup complet): {output_path}")
    
    # Fermer connexion MongoDB si applicable
    if db_service and hasattr(db_service, 'close'):
        db_service.close()
    
    # Nettoyer chunks temporaires apr√®s export r√©ussi
    cleanup_processed_chunks(config, keep_chunks=False)
    
    print(f"\n‚úì {exported_count} types de m√©triques export√©s vers {db_type.upper()}")
    print("\nüí° Pour g√©n√©rer le rapport quotidien (instance s√©par√©e), ex√©cutez:")
    print(f"   python report_generator/main.py {date}")


def main(date: Optional[str] = None):
    """
    Point d'entr√©e principal
    
    Args:
        date: Date au format YYYY-MM-DD (d√©faut: aujourd'hui)
    """
    # D√©terminer la date de traitement
    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")
    else:
        # Valider le format de date
        try:
            datetime.strptime(date, "%Y-%m-%d")
        except ValueError:
            print(f"‚ö† Format de date invalide: {date}, utilisation de la date d'aujourd'hui")
            date = datetime.now().strftime("%Y-%m-%d")
    
    print("=" * 60)
    print("CityFlow Analytics - Traitement des Donn√©es")
    print(f"Date: {date}")
    print("=" * 60)
    
    try:
        # 1. Chargement configuration
        print("\n[1/6] Chargement configuration...")
        config = settings
        print("‚úì Configuration charg√©e")
        
        # 2. Initialisation processeurs
        print("\n[2/6] Initialisation processeurs...")
        processors = initialize_processors(config)
        print(f"‚úì {len(processors)} processeurs initialis√©s")
        
        # 3. Chargement donn√©es brutes
        print("\n[3/6] Chargement donn√©es brutes...")
        raw_data = load_raw_data(config)
        
        data_loaded = sum(1 for v in raw_data.values() if v is not None)
        print(f"‚úì {data_loaded} sources de donn√©es charg√©es")
        
        # 4. Traitement par type de donn√©es
        print("\n[4/6] Traitement des donn√©es...")
        results = {}
        
        # Traiter r√©f√©rentiel en premier (pour enrichissement)
        if raw_data.get("referentiel"):
            print("  ‚Üí Traitement r√©f√©rentiel g√©ographique...")
            results["referentiel"] = processors["referentiel"].process(raw_data["referentiel"])
        
        # Traiter autres donn√©es
        for data_type, processor in processors.items():
            if data_type == "referentiel":
                continue  # D√©j√† trait√©
            
            data = raw_data.get(data_type)
            if data is None:
                print(f"  ‚ö† Pas de donn√©es pour {data_type}")
                continue
            
            print(f"  ‚Üí Traitement {data_type}...")
            
            try:
                # Cas sp√©cial pour comptages (gros fichier)
                if data_type == "comptages" and isinstance(data, str):
                    result = processors[data_type].process_large_file(data)
                else:
                    result = processor.process(data)
                
                results[data_type] = result
                print(f"    ‚úì {data_type} trait√© avec succ√®s")
            except Exception as e:
                print(f"    ‚úó Erreur traitement {data_type}: {e}")
                results[data_type] = {"success": False, "errors": [str(e)]}
        
        # 5. Enrichissement multi-sources
        print("\n[5/6] Enrichissement multi-sources...")
        referentiel_data = results.get("referentiel")
        results = enrich_multi_source(results, referentiel_data)
        print("‚úì Enrichissement termin√©")
        
        # 6. Export r√©sultats (m√©triques uniquement)
        print("\n[6/6] Export des m√©triques...")
        export_results(results, config, date=date)
        print("‚úì Export termin√©")
        
        print("\n" + "=" * 60)
        print("Traitement termin√© avec succ√®s!")
        print("=" * 60)
        db_type = get_database_type()
        print(f"\nüìä M√©triques export√©es dans {db_type.upper()}")
        print("üìã Pour g√©n√©rer le rapport (instance s√©par√©e), ex√©cutez:")
        print("   python report_generator/main.py")
        print("=" * 60)
        
        return results
    
    except Exception as e:
        print(f"\n‚úó ERREUR FATALE: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = main()

